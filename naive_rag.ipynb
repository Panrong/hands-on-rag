{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-28 15:08:01--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/naive_rag/paul_graham_essay.txt’\n",
      "\n",
      "data/naive_rag/paul 100%[===================>]  73.28K  9.28KB/s    in 67s     \n",
      "\n",
      "2024-06-28 15:09:11 (1.09 KB/s) - ‘data/naive_rag/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" -O \"data/paul_graham_essay.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a probe\n",
    "! echo \"SigAIoT will discuss RAG on the morning of July 4th, 2024.\" > data/probe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common configs\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "from utils_fn.helpers import print_qa\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "txt_file_path = \"data/paul_graham_essay.txt\" \n",
    "probe_file_path = \"data/probe.txt\"\n",
    "gpt35_llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# ingestion\n",
    "documents = SimpleDirectoryReader(input_files=[txt_file_path, probe_file_path]).load_data()\n",
    "nodes = IngestionPipeline(transformations=[SentenceSplitter(chunk_size=256, chunk_overlap=0)]).run(documents)\n",
    "# index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# query\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "query_engine = index.as_query_engine(similarity_top_k=5, llm=gpt35_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mQuestion:\u001b[0m what were things that the author worked on before college? \n",
      "\u001b[92mLLM:\u001b[0m Before college, the author worked on various projects and activities such as writing for the school newspaper, participating in debate club, volunteering at a local animal shelter, and interning at a marketing firm. They also worked part-time jobs at a grocery store and a coffee shop to save money for college. \n",
      "\u001b[92mRAG:\u001b[0m The author worked on writing short stories and programming, particularly on an IBM 1401 using an early version of Fortran during 9th grade. Later on, the author started programming on microcomputers like the TRS-80 and wrote simple games, a rocket prediction program, and a word processor.\n"
     ]
    }
   ],
   "source": [
    "q1 = \"what were things that the author worked on before college?\"\n",
    "print_qa(q1, gpt35_llm.complete(q1), query_engine.query(q1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mQuestion:\u001b[0m What will be discussed on 2024-7-4? \n",
      "\u001b[92mLLM:\u001b[0m It is difficult to predict the specific topics that will be discussed on July 4, 2024, as it will depend on current events, political developments, and cultural trends at that time. However, it is likely that discussions on Independence Day in the United States will focus on themes related to freedom, democracy, patriotism, and national identity. Other potential topics could include current political issues, social movements, international relations, and cultural events happening around that time. \n",
      "\u001b[92mRAG:\u001b[0m The discussion on July 4, 2024, will likely revolve around the progress and developments related to the new Lisp language called Bel that was created by Paul Graham in Arc.\n"
     ]
    }
   ],
   "source": [
    "q2 = \"What will be discussed on 2024-7-4?\"\n",
    "print_qa(q2, gpt35_llm.complete(q2), query_engine.query(q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
